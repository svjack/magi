# Magi, The Manga Whisperer

![Static Badge](https://img.shields.io/badge/v1-grey) 
[![Static Badge](https://img.shields.io/badge/arXiv-2401.10224-blue)](http://arxiv.org/abs/2401.10224)
[![Dynamic JSON Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fhuggingface.co%2Fapi%2Fmodels%2Fragavsachdeva%2Fmagi%3Fexpand%255B%255D%3Ddownloads%26expand%255B%255D%3DdownloadsAllTime&query=%24.downloadsAllTime&label=%F0%9F%A4%97%20Downloads)](https://huggingface.co/ragavsachdeva/magi)
[![Static Badge](https://img.shields.io/badge/%F0%9F%A4%97%20Spaces-Demo-blue)](https://huggingface.co/spaces/ragavsachdeva/the-manga-whisperer/)

![Static Badge](https://img.shields.io/badge/v2-grey) 
[![Static Badge](https://img.shields.io/badge/arXiv-2408.00298-blue)](https://arxiv.org/abs/2408.00298)
[![Dynamic JSON Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fhuggingface.co%2Fapi%2Fmodels%2Fragavsachdeva%2Fmagiv2%3Fexpand%255B%255D%3Ddownloads%26expand%255B%255D%3DdownloadsAllTime&query=%24.downloadsAllTime&label=%F0%9F%A4%97%20Downloads)](https://huggingface.co/ragavsachdeva/magiv2)
[![Static Badge](https://img.shields.io/badge/%F0%9F%A4%97%20Spaces-Demo-blue)](https://huggingface.co/spaces/ragavsachdeva/Magiv2-Demo)

```bash
sudo apt-get update && sudo apt-get install cbm ffmpeg git-lfs
```

- äººç‰©åç§°åˆ¤å®šä¸ç²¾ç¡®çš„ç‰ˆæœ¬ï¼ˆå¯ä»¥è€ƒè™‘ä½¿ç”¨tagè¿›è¡Œæ”¹è¿›ï¼‰
```python
#!/usr/bin/env python
# coding: utf-8

# å®‰è£…ä¾èµ–
#get_ipython().system('pip install torch datasets huggingface_hub transformers scipy einops pulp shapely timm')

# å¯¼å…¥å¿…è¦çš„åº“
from datasets import load_dataset
import os
from PIL import Image
import numpy as np
from transformers import AutoModel
import torch

# é…ç½®è¾“å‡ºè·¯å¾„
output_dir = "first_sTitle_output_pages"  # å¯é…ç½®çš„è¾“å‡ºè·¯å¾„
os.makedirs(output_dir, exist_ok=True)  # ç¡®ä¿è·¯å¾„å­˜åœ¨

# åŠ è½½ã€ŠåŸç¥ã€‹æ¼«ç”»çš„è‹±æ–‡æ•°æ®é›†
ds = load_dataset("svjack/Genshin-Impact-Manga-EN-US")

# åŠ è½½ã€ŠåŸç¥ã€‹è§’è‰²æ’å›¾æ•°æ®é›†
Genshin_Impact_Illustration_ds = load_dataset("svjack/Genshin-Impact-Illustration")["train"]
ds_size = len(Genshin_Impact_Illustration_ds)
name_image_dict = {}
for i in range(ds_size):
    row_dict = Genshin_Impact_Illustration_ds[i]
    name_image_dict[row_dict["name"]] = row_dict["image"]

# ä¸­è‹±æ–‡æ˜ å°„å­—å…¸
name_mapping = {
    'å¡é½å¨œ': 'Kachina',
    'ç›æ‹‰å¦®': 'Maranee',
    'é‚£ç»´è±ç‰¹': 'Navilette',
    'è²ç±³å°¼': 'Ferminet',
    'å¨œç»´å¨…': 'Navia',
    'é˜¿è•¾å¥‡è¯º': 'Arlecchino',
    'å¤æ²ƒè•¾': 'Chevreuse',
    'å…‹æ´›ç³å¾·': 'Clorinde',
    'æ—å°¼': 'Lyney',
    'ç³å¦®ç‰¹': 'Lynette',
    'å¸Œæ ¼é›¯': 'Sigewinne',
    'å¤æ´›è’‚': 'Charlotte',
    'èŠ™å®å¨œ': 'Furina',
    'åƒç»‡': 'Chiori',
    'è±æ¬§æ–¯åˆ©': 'Wriothesley',
    'è‰¾æ¢…è‰åŸƒ': 'Emilie',
    'çéœ²çŠ': 'Faruzan',
    'çº³è¥¿å¦²': 'Nahida',
    'å¡ç»´': 'Kaveh',
    'å¦®éœ²': 'Nilou',
    'å¤šè‰': 'Dori',
    'åè’‚ä¸': 'Candace',
    'è¿ªå¸Œé›…': 'Dehya',
    'æµæµªè€…': 'Wanderer',
    'æçº³é‡Œ': 'Tighnari',
    'è‰¾å°”æµ·æ£®': 'Alhaitham',
    'èµ›ç´¢æ–¯': 'Sethos',
    'èµ›è¯º': 'Cyno',
    'æŸ¯è±': 'Collei',
    'è±ä¾æ‹‰': 'Layla',
    'è¡Œç§‹': 'Xingqiu',
    'ç”³é¹¤': 'Shenhe',
    'è¾›ç„±': 'Xinyan',
    'ç‘¶ç‘¶': 'Yaoyao',
    'é‡äº‘': 'Chongyun',
    'ä¸ƒä¸ƒ': 'Qiqi',
    'åˆ»æ™´': 'Keqing',
    'å¤œå…°': 'Yelan',
    'é­ˆ': 'Xiao',
    'è¾¾è¾¾åˆ©äºš': 'Tartaglia',
    'ç”˜é›¨': 'Ganyu',
    'äº‘å ‡': 'Yunjin',
    'é¦™è±': 'Xiangling',
    'å˜‰æ˜': 'Gaming',
    'çƒŸç»¯': 'Yanfei',
    'é—²äº‘': 'Xianyun',
    'åŒ—æ–—': 'Beidou',
    'ç™½æœ¯': 'Baizhu',
    'é’Ÿç¦»': 'Zhongli',
    'å‡å…‰': 'Ningguang',
    'èƒ¡æ¡ƒ': 'Hu Tao',
    'é›·ç”µå°†å†›': 'Raiden Shogun',
    'äº”éƒ': 'Gorou',
    'ä¹æ¡è£Ÿç½—': 'Kujou Sara',
    'é¹¿é‡é™¢å¹³è—': 'Shikanoin Heizou',
    'æ—©æŸš': 'Sayu',
    'å…«é‡ç¥å­': 'Yae Miko',
    'ä¹…å²å¿': 'Kuki Shinobu',
    'ç»®è‰¯è‰¯': 'Kirara',
    'çŠç‘šå®«å¿ƒæµ·': 'Sangonomiya Kokomi',
    'æ«åŸä¸‡å¶': 'Kaedehara Kazuha',
    'ç¥é‡Œç»«äºº': 'Kamisato Ayato',
    'ç¥é‡Œç»«å': 'Kamisato Ayaka',
    'å®µå®«': 'Yoimiya',
    'æ‰˜é©¬': 'Thoma',
    'è’æ³·ä¸€æ–—': 'Arataki Itto',
    'ä¸½è': 'Lisa',
    'è«å¨œ': 'Mona',
    'å®‰æŸ': 'Amber',
    'è¿ªå¢å…‹': 'Diluc',
    'å¯è‰': 'Klee',
    'å‡¯äºš': 'Kaeya',
    'è¯ºè‰¾å°”': 'Noelle',
    'è²è°¢å°”': 'Fischl',
    'ç½—èè‰äºš': 'Rosaria',
    'ç ‚ç³–': 'Sucrose',
    'ä¼˜èˆ': 'Eula',
    'ç´': 'Jean',
    'èŠ­èŠ­æ‹‰': 'Barbara',
    'ç±³å¡': 'Mika',
    'åŸƒæ´›ä¼Š': 'Aloy',
    'ç­å°¼ç‰¹': 'Bennett',
    'é˜¿è´å¤š': 'Albedo',
    'è¿ªå¥¥å¨œ': 'Diona',
    'æ¸©è¿ª': 'Venti',
    'é›·æ³½': 'Razor'
}

# å°† name_image_dict çš„é”®æ›¿æ¢ä¸ºè‹±æ–‡
name_image_dict_en = {name_mapping[name]: image for name, image in name_image_dict.items()}

# è·å–ç¬¬ä¸€ç« çš„æ ‡é¢˜
first_sTitle = ds["train"][0]["sTitle"]
print(f"First chapter title: {first_sTitle}")

# æå–ç¬¬ä¸€ç« çš„æ‰€æœ‰é¡µé¢å›¾ç‰‡
chapter_pages_im = []
for i in range(len(ds["train"])):
    if ds["train"][i]["sTitle"] == first_sTitle:
        chapter_pages_im.append(ds["train"][i]["image"])

print(f"Number of pages in the first chapter: {len(chapter_pages_im)}")

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model = AutoModel.from_pretrained("ragavsachdeva/magiv2", trust_remote_code=True).cuda().eval()

# è¯»å–å›¾ç‰‡çš„å‡½æ•°
def read_image(image):
    # å°† PIL.Image è½¬æ¢ä¸º RGB æ ¼å¼çš„ numpy æ•°ç»„
    image = image.convert("L").convert("RGB")
    image = np.array(image)
    return image

# ä½¿ç”¨ chapter_pages_im ä¸­çš„å›¾ç‰‡ä½œä¸ºæ¼«ç”»é¡µé¢
chapter_pages = [read_image(image) for image in chapter_pages_im]

# ä½¿ç”¨ genshin_impact_images ä¸­çš„å›¾ç‰‡ä½œä¸ºè§’è‰²å›¾ç‰‡
character_bank = {
    "images": [read_image(image) for image in name_image_dict_en.values()],  # è§’è‰²å›¾ç‰‡
    "names": list(name_image_dict_en.keys())  # è§’è‰²åç§°ï¼ˆå›¾ç‰‡è·¯å¾„ä¸­çš„åç§°ï¼‰
}

# ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹
with torch.no_grad():
    per_page_results = model.do_chapter_wide_prediction(chapter_pages, character_bank, use_tqdm=True, do_ocr=True)

# ç”Ÿæˆå¯¹è¯æ–‡æœ¬
transcript = []
for i, (image, page_result) in enumerate(zip(chapter_pages, per_page_results)):
    # å¯è§†åŒ–é¢„æµ‹ç»“æœå¹¶ä¿å­˜ä¸ºå›¾ç‰‡
    output_path = os.path.join(output_dir, f"page_{i}.png")
    model.visualise_single_image_prediction(image, page_result, output_path)
    print(f"Saved visualization to {output_path}")
    
    # è·å–è¯´è¯è€…åç§°
    speaker_name = {
        text_idx: page_result["character_names"][char_idx] for text_idx, char_idx in page_result["text_character_associations"]
    }
    
    # éå†æ¯ä¸€é¡µçš„ OCR ç»“æœ
    for j in range(len(page_result["ocr"])):
        if not page_result["is_essential_text"][j]:
            continue
        name = speaker_name.get(j, "unsure")  # å¦‚æœæ‰¾ä¸åˆ°å¯¹åº”çš„è§’è‰²åç§°ï¼Œä½¿ç”¨ "unsure"
        transcript.append(f"<{name}>: {page_result['ocr'][j]}")

# å°†å¯¹è¯æ–‡æœ¬ä¿å­˜åˆ°æ–‡ä»¶
transcript_path = os.path.join(output_dir, "transcript.txt")
with open(transcript_path, "w", encoding="utf-8") as fh:
    for line in transcript:
        fh.write(line + "\n")

print(f"Transcript saved to {transcript_path}")
```

- ä½¿ç”¨tag ç²¾ç¡®åŒ–åˆ¤å®šäººç‰©åç§°çš„ç‰ˆæœ¬ï¼ˆå¯èƒ½åç»­è¿˜éœ€è¦åŠ å…¥bbox æ˜¯å¦ä¸ºäºº ç­‰çš„åˆ¤å®šï¼Œä½¿ç”¨tagæˆ–è€…é¢ç§¯æ¯”ç‡ç­‰ï¼‰
- ä¹Ÿå¯ä»¥è€ƒè™‘ä½¿å¾—å›¾ç‰‡ä¸æ˜¯ç°åº¦çš„
```python
#!/usr/bin/env python
# coding: utf-8

# å®‰è£…ä¾èµ–
#get_ipython().system('pip install torch datasets huggingface_hub transformers scipy einops pulp shapely timm "dghs-imgutils[gpu]"')

# å¯¼å…¥å¿…è¦çš„åº“
from datasets import load_dataset
import os
from PIL import Image
import numpy as np
from transformers import AutoModel
import torch
from imgutils.tagging import get_wd14_tags  # å¯¼å…¥æ ‡ç­¾é¢„æµ‹å‡½æ•°

# é…ç½®è¾“å‡ºè·¯å¾„
output_dir = "first_sTitle_output_tag_pages"  # å¯é…ç½®çš„è¾“å‡ºè·¯å¾„
os.makedirs(output_dir, exist_ok=True)  # ç¡®ä¿è·¯å¾„å­˜åœ¨

# åŠ è½½ã€ŠåŸç¥ã€‹æ¼«ç”»çš„è‹±æ–‡æ•°æ®é›†
ds = load_dataset("svjack/Genshin-Impact-Manga-EN-US")

# è·å–ç¬¬ä¸€ç« çš„æ ‡é¢˜
first_sTitle = ds["train"][0]["sTitle"]
print(f"First chapter title: {first_sTitle}")

# æå–ç¬¬ä¸€ç« çš„æ‰€æœ‰é¡µé¢å›¾ç‰‡
chapter_pages_im = []
for i in range(len(ds["train"])):
    if ds["train"][i]["sTitle"] == first_sTitle:
        chapter_pages_im.append(ds["train"][i]["image"])

print(f"Number of pages in the first chapter: {len(chapter_pages_im)}")

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model = AutoModel.from_pretrained("ragavsachdeva/magiv2", trust_remote_code=True).cuda().eval()

# è¯»å–å›¾ç‰‡çš„å‡½æ•°
def read_image(image, grayscale=False):
    # å°† PIL.Image è½¬æ¢ä¸º RGB æ ¼å¼çš„ numpy æ•°ç»„
    if grayscale:
        image = image.convert("L").convert("RGB")  # è½¬æ¢ä¸ºç°åº¦å›¾
    else:
        image = image.convert("RGB")  # ä¿æŒåŸè‰²å›¾
    image = np.array(image)
    return image

# ä½¿ç”¨ chapter_pages_im ä¸­çš„å›¾ç‰‡ä½œä¸ºæ¼«ç”»é¡µé¢
chapter_pages_grayscale = [read_image(image, grayscale=True) for image in chapter_pages_im]  # ç°åº¦å›¾ç”¨äºæ¨æ–­
chapter_pages_color = [read_image(image, grayscale=False) for image in chapter_pages_im]  # åŸè‰²å›¾ç”¨äºç»˜åˆ¶

# ä½¿ç”¨ç©ºå­—å…¸ä½œä¸ºè§’è‰²å›¾ç‰‡åº“
character_bank = {
    "images": [], "names": []
}

# ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼ˆä½¿ç”¨ç°åº¦å›¾ï¼‰
with torch.no_grad():
    per_page_results = model.do_chapter_wide_prediction(chapter_pages_grayscale, character_bank, use_tqdm=True, do_ocr=True)

# ç”Ÿæˆå¯¹è¯æ–‡æœ¬
transcript = []
for i, (image, page_result) in enumerate(zip(chapter_pages_color, per_page_results)):  # ä½¿ç”¨åŸè‰²å›¾è¿›è¡Œç»˜åˆ¶
    # æ›´æ–°è§’è‰²åç§°
    for idx, bbox in enumerate(page_result["characters"]):
        # è£å‰ªå›¾ç‰‡
        x1, y1, x2, y2 = map(int, bbox)  # å°† bbox è½¬æ¢ä¸ºæ•´æ•°
        cropped_image = image[y1:y2, x1:x2]  # è£å‰ªå›¾ç‰‡

        # å°†è£å‰ªåçš„å›¾ç‰‡è½¬æ¢ä¸º PIL.Image å¯¹è±¡
        cropped_image_pil = Image.fromarray(cropped_image)

        # ä½¿ç”¨ get_wd14_tags è¿›è¡Œæ ‡ç­¾é¢„æµ‹
        rating, features, chars = get_wd14_tags(cropped_image_pil)
        chars = dict(filter(lambda t2: "genshin_impact" in t2[0].lower(), chars.items()))

        # è·å–å¾—åˆ†æœ€é«˜çš„æ ‡ç­¾
        if chars:
            best_tag = max(chars, key=chars.get)
            page_result["character_names"][idx] = best_tag  # æ›´æ–°è§’è‰²åç§°

    # æ‰“å°æ›´æ–°åçš„è§’è‰²åç§°
    print(f"Updated character names for page {i}: {page_result['character_names']}")

    # è·å–è¯´è¯è€…åç§°
    speaker_name = {
        text_idx: page_result["character_names"][char_idx] for text_idx, char_idx in page_result["text_character_associations"]
    }
    
    # éå†æ¯ä¸€é¡µçš„ OCR ç»“æœ
    for j in range(len(page_result["ocr"])):
        if not page_result["is_essential_text"][j]:
            continue
        name = speaker_name.get(j, "unsure")  # å¦‚æœæ‰¾ä¸åˆ°å¯¹åº”çš„è§’è‰²åç§°ï¼Œä½¿ç”¨ "unsure"
        transcript.append(f"<{name}>: {page_result['ocr'][j]}")

    # å¯è§†åŒ–é¢„æµ‹ç»“æœå¹¶ä¿å­˜ä¸ºå›¾ç‰‡ï¼ˆä½¿ç”¨åŸè‰²å›¾ï¼‰
    output_path = os.path.join(output_dir, f"page_{i}.png")
    model.visualise_single_image_prediction(image, page_result, output_path)
    print(f"Saved visualization to {output_path}")

# å°†å¯¹è¯æ–‡æœ¬ä¿å­˜åˆ°æ–‡ä»¶
transcript_path = os.path.join(output_dir, "transcript.txt")
with open(transcript_path, "w", encoding="utf-8") as fh:
    for line in transcript:
        fh.write(line + "\n")

print(f"Transcript saved to {transcript_path}")
```

![page_73 (1)](https://github.com/user-attachments/assets/335e6da3-1670-4753-a60a-e746d8eba11b)

- éŸ³ä¹style transformation
```bash
sudo apt-get update && sudo apt-get install git-lfs ffmpeg cbm
pip install git+https://github.com/facebookresearch/audiocraft.git
```

```python
import torchaudio
from audiocraft.models import MusicGen
from audiocraft.data.audio import audio_write

model = MusicGen.get_pretrained('melody')
model.set_generation_params(duration=8)  # generate 8 seconds.

descriptions = ['happy rock', 'energetic EDM', 'sad jazz']

melody, sr = torchaudio.load('bach.mp3')
# generates using the melody from the given audio and the provided descriptions.
wav = model.generate_with_chroma(descriptions, melody[None].expand(3, -1, -1), sr)

for idx, one_wav in enumerate(wav):
    # Will save under {idx}.wav, with loudness normalization at -14 db LUFS.
    audio_write(f'{idx}', one_wav.cpu(), model.sample_rate, strategy="loudness")

from IPython import display
display.Audio("bach.mp3")
display.Audio("0.wav")
display.Audio("1.wav")
display.Audio("2.wav")
```

```python
from IPython import display
display.Video("æ¸©è¿ª.mp3")

import torchaudio
from audiocraft.models import MusicGen
from audiocraft.data.audio import audio_write

model = MusicGen.get_pretrained('melody')
model.set_generation_params(duration=1 * 60 + 24)  # generate 8 seconds.

descriptions = ['happy rock', 'energetic EDM', 'sad jazz']

melody, sr = torchaudio.load('æ¸©è¿ª.mp3')
# generates using the melody from the given audio and the provided descriptions.
wav = model.generate_with_chroma(descriptions, melody[None].expand(3, -1, -1), sr)

for idx, one_wav in enumerate(wav):
    # Will save under {idx}.wav, with loudness normalization at -14 db LUFS.
    audio_write(f'{idx}', one_wav.cpu(), model.sample_rate, strategy="loudness")

#### ffmpeg -i 0.wav -c:v libx264 -tune stillimage -c:a aac -b:a 192k -shortest 0.mp4
```




https://github.com/user-attachments/assets/5ccb2e56-d3c8-417c-926d-e04b3946ba80


- Another Text-to-X (have text to audio/music for sound or music)
- https://github.com/Alpha-VLLM/Lumina-T2X

# Table of Contents
1. [Magiv1](#magiv1)
2. [Magiv2](#magiv2)
3. [Datasets](#datasets)

# Magiv1
- The model is available at ğŸ¤— [HuggingFace Model Hub](https://huggingface.co/ragavsachdeva/magi).
- Try it out for yourself using this ğŸ¤— [HuggingFace Spaces Demo](https://huggingface.co/spaces/ragavsachdeva/the-manga-whisperer/) (no GPU, so slow).
- Basic model usage is provided below. Inspect [this file](https://huggingface.co/ragavsachdeva/magi/blob/main/modelling_magi.py) for more info.

![Magi_teaser](https://github.com/ragavsachdeva/magi/assets/26804893/0a6d44bc-12ef-4545-ab9b-577c77bdfd8a)

### v1 Usage
```python
from transformers import AutoModel
import numpy as np
from PIL import Image
import torch
import os

images = [
        "path_to_image1.jpg",
        "path_to_image2.png",
    ]

def read_image_as_np_array(image_path):
    with open(image_path, "rb") as file:
        image = Image.open(file).convert("L").convert("RGB")
        image = np.array(image)
    return image

images = [read_image_as_np_array(image) for image in images]

model = AutoModel.from_pretrained("ragavsachdeva/magi", trust_remote_code=True).cuda()
with torch.no_grad():
    results = model.predict_detections_and_associations(images)
    text_bboxes_for_all_images = [x["texts"] for x in results]
    ocr_results = model.predict_ocr(images, text_bboxes_for_all_images)

for i in range(len(images)):
    model.visualise_single_image_prediction(images[i], results[i], filename=f"image_{i}.png")
    model.generate_transcript_for_single_image(results[i], ocr_results[i], filename=f"transcript_{i}.txt")
```

# Magiv2
- The model is available at ğŸ¤— [HuggingFace Model Hub](https://huggingface.co/ragavsachdeva/magiv2).
- Try it out for yourself using this ğŸ¤— [HuggingFace Spaces Demo](https://huggingface.co/spaces/ragavsachdeva/Magiv2-Demo) (with GPU, thanks HF Team!).
- Basic model usage is provided below. Inspect [this file](https://huggingface.co/ragavsachdeva/magiv2/blob/main/modelling_magiv2.py) for more info.

![magiv2](https://github.com/user-attachments/assets/e0cd1787-4a0c-49a5-a9d8-be2911d5ec08)

### v2 Usage
```python
from PIL import Image
import numpy as np
from transformers import AutoModel
import torch

model = AutoModel.from_pretrained("ragavsachdeva/magiv2", trust_remote_code=True).cuda().eval()


def read_image(path_to_image):
    with open(path_to_image, "rb") as file:
        image = Image.open(file).convert("L").convert("RGB")
        image = np.array(image)
    return image

chapter_pages = ["page1.png", "page2.png", "page3.png" ...]
character_bank = {
    "images": ["char1.png", "char2.png", "char3.png", "char4.png" ...],
    "names": ["Luffy", "Sanji", "Zoro", "Ussop" ...]
}

chapter_pages = [read_image(x) for x in chapter_pages]
character_bank["images"] = [read_image(x) for x in character_bank["images"]]

with torch.no_grad():
    per_page_results = model.do_chapter_wide_prediction(chapter_pages, character_bank, use_tqdm=True, do_ocr=True)

transcript = []
for i, (image, page_result) in enumerate(zip(chapter_pages, per_page_results)):
    model.visualise_single_image_prediction(image, page_result, f"page_{i}.png")
    speaker_name = {
        text_idx: page_result["character_names"][char_idx] for text_idx, char_idx in page_result["text_character_associations"]
    }
    for j in range(len(page_result["ocr"])):
        if not page_result["is_essential_text"][j]:
            continue
        name = speaker_name.get(j, "unsure") 
        transcript.append(f"<{name}>: {page_result['ocr'][j]}")
with open(f"transcript.txt", "w") as fh:
    for line in transcript:
        fh.write(line + "\n")
```

# Datasets

Disclaimer: In adherence to copyright regulations, we are unable to _publicly_ distribute the manga images that we've collected. The test images, however, are available freely, publicly and officially on [Manga Plus by Shueisha](https://mangaplus.shueisha.co.jp/).

[![Static Badge](https://img.shields.io/badge/%F0%9F%A4%97%20%20PopMangaX%20(Test)-Dataset-blue)](https://huggingface.co/datasets/ragavsachdeva/popmanga_test)
[![Static Badge](https://img.shields.io/badge/%F0%9F%A4%97%20%20PopCharacters-Dataset-blue)](https://huggingface.co/datasets/ragavsachdeva/popcharacters)

### Other notes
- Request to download Manga109 dataset [here](http://www.manga109.org/en/download.html).
- Download a large scale dataset from Mangadex using [this tool](https://github.com/EMACC99/mangadex).
- The Manga109 test splits are available here: [detection](https://github.com/barisbatuhan/DASS_Det_Inference/blob/main/dass_det/data/datasets/manga109.py#L109), [character clustering](https://github.com/kktsubota/manga-face-clustering/blob/master/dataset/test_titles.txt). Be careful that some background characters have the same label even though they are not the same character, [see](https://github.com/kktsubota/manga-face-clustering/blob/master/script/get_other_ids.py).



# License and Citation
The provided models and datasets are available for academic research purposes only.

```
@InProceedings{magiv1,
    author    = {Sachdeva, Ragav and Zisserman, Andrew},
    title     = {The Manga Whisperer: Automatically Generating Transcriptions for Comics},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {12967-12976}
}
```
```
@misc{magiv2,
      author={Ragav Sachdeva and Gyungin Shin and Andrew Zisserman},
      title={Tails Tell Tales: Chapter-Wide Manga Transcriptions with Character Names}, 
      year={2024},
      eprint={2408.00298},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2408.00298}, 
}
```
